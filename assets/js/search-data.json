{
  
    
        "post0": {
            "title": "Convolutional Layers",
            "content": "Compassion is a verb. . – Thich Nhat Hanh . Today we will have a first look convolutional layers. Convolutional Neural Networks were first introduced in the 1980s by Yann LeCun and have been at the heart of most image related AI development ever since. While the latest research might change this . An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale I am sure the power of convolutions will remain a powerful techinque for any AI image wizards for quite some time. . A nice concise introduction of convolutional layers can be found on this excellent pyimagesearch blog by Adrian Rosebrock: . https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/ . For a more thorough explanation we refer to Chapter 14 of the Tensorflow bible: . Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow . Here we just start playing around with this technique a little bit basically following the code from the get blog post How to Visualize Filters and Feature Maps in Convolutional Neural Networks by Jason Brownlee . https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/ . Getting ready . import numpy as np import tensorflow as tf from matplotlib import pyplot as plt from tensorflow.keras.applications.vgg16 import VGG16 image_path = &#39;./images/dance.jpg&#39; . Preprocessing the image . Loading the image file into PIL format (target size 224,224): https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/load_img . | Converting the PIL Image instance to a Numpy array: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/img_to_array . | Converting the single image into a batch. (Keras layers are expecting a batch as input.) . | image_pil = tf.keras.preprocessing.image.load_img(image_path,target_size=(224,224)) #(1) image_np_array = tf.keras.preprocessing.image.img_to_array(image_pil) #(2) image_np_array_batch = np.array([image_np_array]) #(3) . The input image . plt.figure(figsize=(12,12)) plt.title(&#39;Input Image&#39;) plt.imshow(image_np_array/255.0) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-11-16T20:33:27.228490 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ Loading the VGG16 Model . Next we will load the VGG16 model and print out summary information. . model = VGG16() model.summary() . Model: &#34;vgg16&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 224, 224, 3)] 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 224, 224, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 112, 112, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 112, 112, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 112, 112, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 56, 56, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 56, 56, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 28, 28, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 14, 14, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 7, 7, 512) 0 _________________________________________________________________ flatten (Flatten) (None, 25088) 0 _________________________________________________________________ fc1 (Dense) (None, 4096) 102764544 _________________________________________________________________ fc2 (Dense) (None, 4096) 16781312 _________________________________________________________________ predictions (Dense) (None, 1000) 4097000 ================================================================= Total params: 138,357,544 Trainable params: 138,357,544 Non-trainable params: 0 _________________________________________________________________ . A closer look at the convolutional layers . print(&quot;{:3s}: {:12s}: {:18s} {} n&quot;.format(&#39;pos&#39;,&#39;layer name&#39;,&#39;filter shape&#39;,&#39;output shape&#39;)) for i,layer in enumerate(model.layers): if &#39;conv&#39; in layer.name: filters, biases = layer.get_weights() print(&quot;{:3d}: {:12s} {:18s} {} &quot;.format(i,layer.name, str(filters.shape),str(layer.output.shape))) . pos: layer name : filter shape output shape 1: block1_conv1 (3, 3, 3, 64) (None, 224, 224, 64) 2: block1_conv2 (3, 3, 64, 64) (None, 224, 224, 64) 4: block2_conv1 (3, 3, 64, 128) (None, 112, 112, 128) 5: block2_conv2 (3, 3, 128, 128) (None, 112, 112, 128) 7: block3_conv1 (3, 3, 128, 256) (None, 56, 56, 256) 8: block3_conv2 (3, 3, 256, 256) (None, 56, 56, 256) 9: block3_conv3 (3, 3, 256, 256) (None, 56, 56, 256) 11: block4_conv1 (3, 3, 256, 512) (None, 28, 28, 512) 12: block4_conv2 (3, 3, 512, 512) (None, 28, 28, 512) 13: block4_conv3 (3, 3, 512, 512) (None, 28, 28, 512) 15: block5_conv1 (3, 3, 512, 512) (None, 14, 14, 512) 16: block5_conv2 (3, 3, 512, 512) (None, 14, 14, 512) 17: block5_conv3 (3, 3, 512, 512) (None, 14, 14, 512) . A sub model of VGG16 . Next we will define a submodel of VGG16 containing only the first convolutional layer . layer_one_model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[1].output) layer_one_model.summary() . Model: &#34;model_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 224, 224, 3)] 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 ================================================================= Total params: 1,792 Trainable params: 1,792 Non-trainable params: 0 _________________________________________________________________ . Visualizing feature maps . Finally the fruit of our efforts, we apply the above defined submodel of VGG16 on our input image to see the 64 feature maps which are the output of the convolutional layer. . feature_maps = layer_one_model.predict(image_np_array_batch) square = 8 ix = 1 plt.figure(figsize=(12,12)) for _ in range(square): for _ in range(square): # specify subplot and turn of axis ax = plt.subplot(square, square, ix) ax.set_xticks([]) ax.set_yticks([]) # plot filter channel in grayscale plt.imshow(feature_maps[0, :, :, ix-1], cmap=&#39;gray&#39;) ix += 1 # show the figure plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-11-16T20:33:30.567787 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/",
            "url": "https://dorjeduck.github.io/ai-candies/2020/11/15/convolutional-layers.html",
            "relUrl": "/2020/11/15/convolutional-layers.html",
            "date": " • Nov 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Don't be mean",
            "content": "My true religion is kindness. . – The 14th Dalai Lama . In this short notebook we will see how to the calculate the pixel wise mean over a collection of images. For this we implement a simple custom Keras Layer which combines each given number of images within the input batch by calculating the mean of the images. . Getting ready . import numpy as np import tensorflow as tf from matplotlib import pyplot as plt from IPython import display image_paths = [] image_paths.append(&#39;./images/lovecoffee.jpg&#39;) image_paths.append(&#39;./images/lovekillscapitalism.jpg&#39;) image_paths.append(&#39;./images/light_1.jpg&#39;) image_paths.append(&#39;./images/light_2.jpg&#39;) . Preprocessing the images . Loading the two image files into PIL format: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/load_img . | Converting the PIL image instancea to Numpy arrays: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/img_to_array . | image_np_arries = [] for image_path in image_paths: image_pil = tf.keras.preprocessing.image.load_img(image_path) #(1) image_np_array = tf.keras.preprocessing.image.img_to_array(image_pil) #(2) image_np_arries.append(image_np_array) image_np_array_batch = np.array(image_np_arries) . The input images . f, axarr = plt.subplots(2,2) f.set_figheight(12) f.set_figwidth(12) for idx, image_np_array in enumerate(image_np_array_batch): row = idx % 2 col = idx // 2 axarr[col,row].imshow(image_np_array/255.) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-11-01T16:32:40.654623 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ Calculating the mean images . As we believe in the good of human kind we assume for the sake of simplicity that the length of the input batch is a multiple of the given images_per_mean parameter. . class ImagesMean(tf.keras.layers.Layer): def __init__(self,images_per_mean): super(ImagesMean, self).__init__() self.images_per_mean = images_per_mean def call(self,input): x = tf.reshape(input,[-1,self.images_per_mean,input.shape[1],input.shape[2],input.shape[3]]) return tf.math.reduce_mean(x,axis = 1) images_mean_layer = ImagesMean(2) image_mean_batch = images_mean_layer(image_np_array_batch) . for image_np_array in image_mean_batch: plt.figure(figsize=(12,12)) plt.imshow(image_np_array/255.0) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-11-01T16:31:11.665155 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-11-01T16:31:12.264362 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/",
            "url": "https://dorjeduck.github.io/ai-candies/2020/11/01/dont-be-mean.html",
            "relUrl": "/2020/11/01/dont-be-mean.html",
            "date": " • Nov 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Patchwork",
            "content": "You exist as an idea of your mind. . – Shunryu Suzuki . After these first pretty boring posts let&#39;s do something a bit more interesting. A while ago I stumbled over . COCO-GAN:Generation by Parts via Conditional Coordinating https://hubert0527.github.io/COCO-GAN/ . The main idea behind this Generative Adverserial Network is to generate images of human faces not directly in their entirety yet as the title suggests by parts. . Inspired by this interesting idea, let&#39;s create one building block of such a GAN by writing Tensorflow Layers which . decompose an image into patches of given size | compose these patches into a single image again | . Getting ready . import numpy as np import tensorflow as tf from matplotlib import pyplot as plt from IPython import display image_path = &#39;./images/sunflower.jpg&#39; . Preprocessing the image . Loading the image file into PIL format: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/load_img . | Converting the PIL Image instance to a Numpy array: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/img_to_array . | Converting the single image into a batch. (Keras layers are expecting a batch as input.) . | image_pil = tf.keras.preprocessing.image.load_img(image_path) #(1) image_np_array = tf.keras.preprocessing.image.img_to_array(image_pil) #(2) image_np_array_batch = np.array([image_np_array]) #(3) print(&quot;Image numpy array shape: {}&quot;.format(image_np_array.shape)) . Image numpy array shape: (1080, 1080, 3) . The original image . plt.figure(figsize=(12,12)) plt.title(&#39;Original Image&#39;) plt.imshow(image_np_array/255.0) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-11-01T13:50:03.184920 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ The Magic . Time to do the interesting part. In order to decompose an image into patches and also be able to put these patches together into a single image again we create two subclasses of the Keras Layer class. The actual image manipulation is a series of reshape and transpose operation on the input data. . tf.transpose https://www.tensorflow.org/api_docs/python/tf/transpose . | tf.reshape https://www.tensorflow.org/api_docs/python/tf/reshape . | . If you are not familiar with these operation I highly recommend to have a look at the examples given in the documentation linked above and play around with them. . The following implemention of the classes ImageToPatches and PatchesToImage include the possiblity to print out a summary of the shape transitions of the input occuring during a call. Normally debugging during implemention would do the job of course. . In our concrete example, we will get the following shape transitions: . ImageToPatches . (1, 1080, 1080, 3) -&gt; (1, 1080, 2, 540, 3) | (1, 1080, 2, 540, 3) -&gt; (1, 2, 1080, 540, 3) | (1, 2, 1080, 540, 3) -&gt; (4, 540, 540, 3) | PatchesToImage . (4, 540, 540, 3) -&gt; (1, 2, 1080, 540, 3) | (1, 2, 1080, 540, 3) -&gt; (1, 1080, 2, 540, 3) | (1, 1080, 2, 540, 3) -&gt; (1, 1080, 1080, 3) | Later on when we are working with Keras Models the summary method of the Model class will provide this information among other useful data. . class Patchwork(tf.keras.layers.Layer): def __init__(self, image_shape, patch_size): super(Patchwork, self).__init__() self.image_shape = image_shape self.patch_size = patch_size def call(self,input): self.shapes = [] self._store_shape(input) return self.process_input(input) def process_input(self, input): return input def print_transition_summary(self): print(&quot; n{} input shape transitions: n&quot;.format(self.__class__.__name__)) if len(self.shapes) &gt; 1: for i in range(1,len(self.shapes)): print(&quot;{}: {} -&gt; {}&quot;.format(i,self.shapes[i-1],self.shapes[i])) print(&quot; n&quot;) def _store_shape(self,arr): self.shapes.append(arr.shape) class ImageToPatches(Patchwork): def __init__(self, image_shape, patch_size): super(ImageToPatches, self).__init__(image_shape,patch_size) def process_input(self, input): batch_size = tf.shape(input)[0] x = tf.reshape(input, [ batch_size, self.image_shape[0], -1, self.patch_size[1], self.image_shape[2]]) self._store_shape(x) x = tf.transpose(x, [0, 2, 1, 3, 4]) self._store_shape(x) x = tf.reshape(x, [-1, self.patch_size[0], self.patch_size[1], self.image_shape[2]]) self._store_shape(x) return x class PatchesToImage(Patchwork): def __init__(self, image_shape, patch_size): super(PatchesToImage, self).__init__(image_shape,patch_size) self.patch_factor = ( self.image_shape[0]//self.patch_size[0]) * (self.image_shape[1]//self.patch_size[1]) def process_input(self, input): result_batch_size = tf.shape(input)[0] / self.patch_factor x = tf.reshape(input, [ result_batch_size, -1, self.image_shape[0], self.patch_size[1], self.image_shape[2]]) self._store_shape(x) x = tf.transpose(x, [0, 2, 1, 3, 4]) self._store_shape(x) x = tf.reshape( x, [-1, self.image_shape[0], self.image_shape[1], self.image_shape[2]]) self._store_shape(x) return x . Decomposing the image . image_shape = image_np_array.shape patch_size = (image_shape[0]//2,image_shape[1]//2) image_to_patches_layer = ImageToPatches(image_shape,patch_size) patches = image_to_patches_layer(image_np_array_batch) image_to_patches_layer.print_transition_summary() . ImageToPatches input shape transitions: 1: (1, 1080, 1080, 3) -&gt; (1, 1080, 2, 540, 3) 2: (1, 1080, 2, 540, 3) -&gt; (1, 2, 1080, 540, 3) 3: (1, 2, 1080, 540, 3) -&gt; (4, 540, 540, 3) . The glorious result . f, axarr = plt.subplots(2,2) f.set_figheight(12) f.set_figwidth(12) for idx, image_np_array in enumerate(patches): row = idx // 2 col = idx % 2 axarr[col,row].imshow(image_np_array/255.) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-11-01T13:50:04.299987 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ Recomposing the original image from the patch . patches_to_image_layer = PatchesToImage(image_shape,patch_size) recomposed_image = patches_to_image_layer(patches) patches_to_image_layer.print_transition_summary() plt.figure(figsize=(12,12)) plt.title(&#39;Recomposed Image&#39;) plt.imshow(recomposed_image[0]/255.0) plt.show() . PatchesToImage input shape transitions: 1: (4, 540, 540, 3) -&gt; (1, 2, 1080, 540, 3) 2: (1, 2, 1080, 540, 3) -&gt; (1, 1080, 2, 540, 3) 3: (1, 1080, 2, 540, 3) -&gt; (1, 1080, 1080, 3) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-11-01T13:50:05.194234 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ Let&#39;s play . Having the patches of the image at our fingertips, why not trying to be funny by changing the order of the patches before putting them together again. . flipped_patches = np.flip(patches,0) flipped_patches_image = patches_to_image_layer(flipped_patches) plt.figure(figsize=(12,12)) plt.title(&#39;Image with flipped patches&#39;) plt.imshow(flipped_patches_image[0]/255.0) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-11-01T13:50:06.058824 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/",
            "url": "https://dorjeduck.github.io/ai-candies/2020/10/31/patchwork.html",
            "relUrl": "/2020/10/31/patchwork.html",
            "date": " • Oct 31, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Image Flip Layer",
            "content": "Mirror facing mirror - nowhere else. . – Ikkyu . In this tutorial we will create our first custom Keras layer which flips the input images horizontal. . Getting ready . import numpy as np import os import tensorflow as tf from matplotlib import pyplot as plt from IPython import display image_paths = [] image_paths.append(&#39;./images/espresso_1.jpg&#39;) image_paths.append(&#39;./images/espresso_2.jpg&#39;) . Preprocessing the images . Loading the two image files into PIL format: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/load_img . | Converting the PIL image instances into Numpy arrays: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/img_to_array . | image_np_arries = [] for image_path in image_paths: image_pil = tf.keras.preprocessing.image.load_img(image_path) #(1) image_np_array = tf.keras.preprocessing.image.img_to_array(image_pil) #(2) image_np_arries.append(image_np_array) image_np_array_batch = np.array(image_np_arries) . The input images . for idx, image_np_array in enumerate(image_np_array_batch): plt.figure(figsize=(12,12)) plt.title(os.path.basename(image_paths[idx])) plt.imshow(image_np_array/255.0) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-31T18:10:43.823761 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-31T18:10:44.367276 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ Our custom Keras layer . For a thorough explanation on custom layers (and models) see: . https://www.tensorflow.org/guide/keras/custom_layers_and_models . class HorizontalFlip(tf.keras.layers.Layer): def __init__(self): super(HorizontalFlip, self).__init__() def call(self, inputs): return tf.image.flip_left_right(inputs) . Flipping the images . horizontal_flip = HorizontalFlip() image_flipped_np_array_batch = horizontal_flip(image_np_array_batch) . The glorious result . for idx, image_np_array in enumerate(image_flipped_np_array_batch): plt.figure(figsize=(12,12)) plt.title(&quot;flipped {}&quot;.format(os.path.basename(image_paths[idx]))) plt.imshow(image_np_array/255.0) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-31T18:10:45.822297 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-31T18:10:46.356201 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/",
            "url": "https://dorjeduck.github.io/ai-candies/2020/10/26/image-flip-layer.html",
            "relUrl": "/2020/10/26/image-flip-layer.html",
            "date": " • Oct 26, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Image cropping",
            "content": "Swim out of your little pond. . – Rumi . In this short tutorial we will see one way of cropping the central portion of two images to a given target height and width with the use of a Keras layer. It is important to notice that the two images have to be of the same height and width in order to be able to crop them togther in one batch. . Getting ready . import numpy as np import os import tensorflow as tf from matplotlib import pyplot as plt from IPython import display target_height = 200 target_width = 800 image_paths = [] image_paths.append(&#39;./images/boot_ganga_1.jpg&#39;) image_paths.append(&#39;./images/boot_ganga_2.jpg&#39;) . Preprocessing the images . Loading the two image files into PIL format: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/load_img . | Converting the PIL image instancea to Numpy arrays: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/img_to_array . | image_np_arries = [] for image_path in image_paths: image_pil = tf.keras.preprocessing.image.load_img(image_path) #(1) image_np_array = tf.keras.preprocessing.image.img_to_array(image_pil) #(2) image_np_arries.append(image_np_array) height, width, _ = image_np_array.shape print(&quot;height/width of {} : {}/{}&quot;.format(os.path.basename(image_path),image_np_array.shape[0],image_np_array.shape[1])) image_np_array_batch = np.array(image_np_arries) . height/width of boot_ganga_1.jpg : 800/1200 height/width of boot_ganga_2.jpg : 800/1200 . The input images . for idx, image_np_array in enumerate(image_np_array_batch): plt.figure(figsize=(12,12)) plt.title(os.path.basename(image_paths[idx])) plt.imshow(image_np_array/255.0) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-31T17:31:29.613095 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-31T17:31:30.036683 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ Cropping the images . 1) Creating a CenterCrop layer for given target height and width. https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/CenterCrop . 2) Cropping the images . center_crop_layer = tf.keras.layers.experimental.preprocessing.CenterCrop(target_height,target_width) #(1) image_cropped_np_array_batch = center_crop_layer(image_np_array_batch) #(2) print(&quot;Keras CenterCrop Layer output shape (Numpy Array): {}&quot;.format(image_cropped_np_array_batch.shape)) . Keras CenterCrop Layer output shape (Numpy Array): (2, 200, 800, 3) . The glorious result . for idx, image_cropped_np_array in enumerate(image_cropped_np_array_batch): plt.figure(figsize=(8,8)) plt.title(&quot;resized {}&quot;.format(os.path.basename(image_paths[idx]))) plt.imshow(image_cropped_np_array/255.0) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-31T17:31:31.261107 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-31T17:31:31.435318 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/",
            "url": "https://dorjeduck.github.io/ai-candies/2020/10/26/image-center-crop.html",
            "relUrl": "/2020/10/26/image-center-crop.html",
            "date": " • Oct 26, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Resizing an image with a Keras Layer",
            "content": "Out real nature is infinite in scope. . – Mingyur Rinpoche . First post, everyone welcome. As a start we will use a Keras Layer to resize an image. Sounds doable but do we need a deep learning library like Keras/Tensorflow for that? No. Yet this blog is about learning the building blocks for complex neural networks. . Enjoy . Getting ready . import numpy as np import tensorflow as tf from matplotlib import pyplot as plt from IPython import display image_path = &#39;./images/fujiyama.png&#39; . Preprocessing the image . Loading the image file into PIL format: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/load_img . | Converting the PIL Image instance to a Numpy array: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/img_to_array . | Determining the height and width of the image. . | Converting the single image into a batch. (Keras layers are expecting a batch as input.) . | image_pil = tf.keras.preprocessing.image.load_img(image_path) #(1) image_np_array = tf.keras.preprocessing.image.img_to_array(image_pil) #(2) height,width,_ = image_np_array.shape #(3) image_np_array_batch = np.array([image_np_array]) #(4) print(&quot;Image numpy array shape: {}&quot;.format(image_np_array.shape)) . Image numpy array shape: (1024, 1536, 3) . The input image . plt.figure(figsize=(12,12)) plt.title(&#39;Input Image&#39;) plt.imshow(image_np_array/255.0) plt.show() print(&quot; nImage height/width: {}/{} n&quot;.format(height,width)) . Image height/width: 1024/1536 . Resizing the image . 1) Creating an Image resizing layer for given target height &amp; width with Keras https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Resizing . 2) Resizing the image . height_resized = height // 2 width_resized = width // 2 print(&quot;Desired target image height/width: {}/{}&quot;.format(height_resized,width_resized)) resize_layer = tf.keras.layers.experimental.preprocessing.Resizing(height_resized,width_resized,interpolation=&#39;bilinear&#39;) #(1) image_resized_np_array_batch = resize_layer(image_np_array_batch) #(2) print(&quot;Keras Resizing Layer output shape (Numpy Array): {}&quot;.format(image_resized_np_array_batch.shape)) . Desired target image height/width: 512/768 Keras Resizing Layer output shape (Numpy Array): (1, 512, 768, 3) . The glorious result . image_resized_np_array = image_resized_np_array_batch[0] height_resized,width_resized,_ = image_resized_np_array.shape plt.figure(figsize=(6,6)) plt.title(&#39;Resized Image&#39;) plt.imshow(image_resized_np_array/255.0) plt.show() print(&quot; nResized image height/width: {}/{} n&quot;.format(height_resized,width_resized)) . Resized image height/width: 512/768 .",
            "url": "https://dorjeduck.github.io/ai-candies/2020/10/25/image-resizing.html",
            "relUrl": "/2020/10/25/image-resizing.html",
            "date": " • Oct 25, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Hello visual world",
          "content": "I study my mind and therefore all appearances are my texts. . – Milarepa . This blog is driven by the idea to teach myself how to process and generate images with Tensorflow. The initial posts will mostly cover basic techniques which hopefully come in handy years later when I start trying to impress myself with more complex models based on these earlier insights. . Please feel invited to come along with me on this journey. I will try to include links to the respective parts of the excellent Tensorflow documentation or other gems on the web as much as possible in order to make this whole endeavour at least of some benefit. . As always, feedback and suggestions of any kind are highly appreciated. .",
          "url": "https://dorjeduck.github.io/ai-candies/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://dorjeduck.github.io/ai-candies/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}